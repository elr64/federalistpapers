{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTSM Text Generation for the Federalist Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the ability of a recurrent neural network built with two long term short memory to accurately produce text mimicking the Federalist Papers. The model is trained on all of the papers, just the papers written by Hamilton, just the papers written by Madison, and the papers written by Madison combined with the disputed 12 papers now attributed to Madison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed packages\n",
    "import argparse\n",
    "import re\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the papers must be read in. The number, author, and  text is extracted from each individual text file. The text is stripped to just the body of the text by splitting the text on the opening line and signature of every paper and any punctuation is separated with a space, so that once the text is tokenized the recurrent neural network will learn punctuation and words will not be embedded multiple times (i.e. with and without punctuation). A tab-delimited text file is created containing the paper number, author, and cleaned body text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getall_papers(file):\n",
    "    all_papers=open(file, \"w\")\n",
    "    writer=csv.writer(all_papers, delimiter='\\t')\n",
    "    for i in range(1,87):\n",
    "        with open(f\"federalistpapers/papers/fednum{i}.txt\") as paper:\n",
    "            paper=paper.read()\n",
    "            paper=paper.replace('\\n', ' ')\n",
    "            text=re.split(r'To the People of the State of New York', paper)\n",
    "            strip=re.split(r'PUBLIUS', text[1])\n",
    "            #add spaces between words and punctuation\n",
    "            strip[0]=re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', strip[0])\n",
    "            if \"HAMILTON OR MADISON\" in text[0]:\n",
    "                writer.writerow([i, \"Unknown\", strip[0]])\n",
    "            elif \"58\" in text[0]: #Project Gutenberg classifies this disputed paper as Madison\n",
    "                writer.writerow([i, \"Unknown\", strip[0]])\n",
    "            elif \"HAMILTON AND MADISON\" in text[0]: #ignore collaborated texts\n",
    "                writer.writerow([i, \"HamiltonandMadison\", strip[0]])\n",
    "            elif \"HAMILTON\" in text[0]:\n",
    "                writer.writerow([i, \"Hamilton\", strip[0]])\n",
    "            elif \"MADISON\" in text[0]:\n",
    "                writer.writerow([i, \"Madison\", strip[0]])\n",
    "            elif \"JAY\" in text[0]:\n",
    "                writer.writerow([i, \"Jay\", strip[0]])\n",
    "    all_papers=open(file, \"r\")\n",
    "    return all_papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the tsv file lists of authors and their corresponding papers are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authortext(all_papers):\n",
    "    authors=[]\n",
    "    papers=[]\n",
    "    numbers=[]\n",
    "    for line in all_papers:\n",
    "        fields = line.strip().split(\"\\t\")\n",
    "        authors.append(fields[1])\n",
    "        papers.append(fields[2])\n",
    "        numbers.append(fields[0])\n",
    "    return authors, papers, numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lists are then used to create singular lists for Hamilton, Madison, Madison plus the disputed papers, and all of the papers that contain strings for each paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_authors(authors, papers):\n",
    "    hamilton=[]\n",
    "    madison=[]\n",
    "    madisonpred=[]\n",
    "    allpapers=[]\n",
    "    for i,author in enumerate(authors):\n",
    "        if author == 'Hamilton':\n",
    "            hamilton.append(papers[i].strip('\":')) #removing leading \": from text\n",
    "        elif author == 'Madison':\n",
    "            madison.append(papers[i].strip('\":'))\n",
    "            madisonpred.append(papers[i].strip('\":'))\n",
    "        elif author == 'Unknown':\n",
    "            madisonpred.append(papers[i].strip('\":'))\n",
    "        allpapers.append(papers[i].strip('\":'))\n",
    "    return hamilton, madison, madisonpred, allpapers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recurrent neural network function reads in a list of strings of text. It then processes these texts for use in training a recurrent neural network. The preprocessing includes tokenizing and indexing the words, creating sequences of words and feature/label matrices. The features and labels are then split into training and validation, so that the quality of the model can be analyzed. A recurrent neural network is created with an embedding layer that creates word embeddings based on the vocabulary of the text and two long short-term memory layers, with appropriate dropout to prevent overtraining. The model stops training once the validation loss stops decreasing. Accuracy and perplexity are then calculated for the model. The model is then used to generate 3 500 word strings of text. These strings are printed and then the list of them is returned by the model.\n",
    "\n",
    "The following code is adapted from https://github.com/WillKoehrsen/recurrent-neural-networks/blob/master/notebooks/Deep%20Dive%20into%20Recurrent%20Neural%20Networks.ipynb which provides an extensive outline on how to set up an LSTM recurrent neural network using Keras to generate text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model(papers):\n",
    "    #create and fit tokenizer on formatted papers    \n",
    "    tokenizer=Tokenizer(num_words=None, filters='\"#$%&()*+/<=>?@[\\\\]^_`{|}~\\t\\n', lower=False, \n",
    "                        split=' ', char_level=False)\n",
    "    tokenizer.fit_on_texts(papers)\n",
    "    \n",
    "    #create lookup and reverse lookup dictionaries\n",
    "    word_idx = tokenizer.word_index\n",
    "    idx_word = tokenizer.index_word\n",
    "    num_words=len(word_idx) + 1\n",
    "    \n",
    "    #convert text to sequences of integers & make features and labels\n",
    "    sequences=tokenizer.texts_to_sequences(papers)\n",
    "    features=[]\n",
    "    labels=[]\n",
    "    for sequence in sequences:\n",
    "        for i in range(50, len(sequence)):\n",
    "            extract = sequence[i - 50:i + 1]\n",
    "            features.append(extract[:-1])\n",
    "            labels.append(extract[-1])  \n",
    "    features=np.array(features)\n",
    "    \n",
    "    #one hot encode labels\n",
    "    labels_arr=np.zeros((len(labels), num_words), dtype=np.int)\n",
    "    for i, word_index in enumerate(labels):\n",
    "        labels_arr[i, word_index] = 1\n",
    "        \n",
    "    #split into training and validation\n",
    "    features, labels_arr=shuffle(features, labels_arr)\n",
    "    idx=int(len(labels)*.8)\n",
    "    train_x = features[:idx]\n",
    "    train_y=labels_arr[:idx]\n",
    "    valid_x=features[idx:]\n",
    "    valid_y=labels_arr[idx:]\n",
    "    \n",
    "    #define perplexity to assess quality of models\n",
    "    def perplexity(y_true, y_pred):\n",
    "        cross_entropy = K.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "        perplexity=K.exp(cross_entropy)\n",
    "        return perplexity\n",
    "\n",
    "    #set up model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, 100, input_length=50, trainable=True))\n",
    "    model.add(LSTM(256, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
    "    model.add(LSTM(256, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_words, activation='softmax'))\n",
    "    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy', perplexity])\n",
    "    \n",
    "    #define callbacks to stop training once validation loss stops decreasing\n",
    "    def make_callbacks(model):\n",
    "        callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "        callbacks.append(ModelCheckpoint('{papers}model.h5',save_best_only=True, \n",
    "                        save_weights_only=False)) #saves best model as loss decreases \n",
    "        return callbacks\n",
    "    callbacks = make_callbacks(model)\n",
    "    \n",
    "    #train model\n",
    "    model.fit(train_x, train_y, epochs=150, batch_size=2048, verbose=0, callbacks=callbacks, validation_data=(valid_x, valid_y))\n",
    " \n",
    "    #evaluate model\n",
    "    model\n",
    "    r = model.evaluate(valid_x, valid_y, batch_size=2048, verbose=1)\n",
    "    valid_accuracy=r[1]\n",
    "    valid_perplexity=r[2]\n",
    "    print(\"Evaluating Model on Validation Data\")\n",
    "    print(f\"Accuracy: {round(100 * valid_accuracy, 2)}%\")\n",
    "    print(f\"Perplexity: {valid_perplexity})\n",
    "    #make predictions\n",
    "    seq = random.choice(sequences) #choose a random sequence\n",
    "    seed_idx = random.randint(0, len(seq) - 60) # choose a random starting point\n",
    "    end_idx = seed_idx + 50 #ending index for seed\n",
    "    \n",
    "    #generate 3 500 word texts\n",
    "    gen_list = []\n",
    "    for n in range(3):\n",
    "        seed = seq[seed_idx:end_idx] #extract the seed sequence\n",
    "        generated = seed[:] + ['#']\n",
    "        for i in range(500): #keep adding new words\n",
    "            # Make a prediction from the seed\n",
    "            preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(np.float64)\n",
    "            preds = np.log(preds)/0.75 #diversify\n",
    "            exp_preds = np.exp(preds)\n",
    "            preds = exp_preds / sum(exp_preds) #softmax\n",
    "            #choose next word\n",
    "            probas = np.random.multinomial(1, preds, 1)[0]\n",
    "            next_idx = np.argmax(probas)\n",
    "            #new seed adds on old word\n",
    "            seed = seed[1:] + [next_idx]\n",
    "            generated.append(next_idx)\n",
    "        # print generated text\n",
    "        gen = []\n",
    "        for i in generated:\n",
    "            gen.append(idx_word.get(i))\n",
    "        #remove spaces between punctuation added earlier\n",
    "        gen=re.sub(r'\\s+([.,;?])', r'\\1', ' '.join(gen))\n",
    "        print(gen)\n",
    "        gen_list.append(gen)\n",
    "    return gen_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function for saving all of the text generated by recurrent neural networks is written to create a tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_generated_text(texts1, texts2, texts3):\n",
    "    outfile=open(\"generated_texts.tsv\", \"w\")\n",
    "    writer=csv.writer(outfile, delimiter='\\t')\n",
    "    for i in texts1:\n",
    "        writer.writerow([f\"{texts1}\", i])\n",
    "    for i in texts2:\n",
    "        writer.writerow([f\"{texts2}\", i])\n",
    "    for i in texts3:\n",
    "        writer.writerow([f\"{texts3}\", i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the above functions are called to read in, clean, and separate the text. Recurrent neural networks are trained on the entirety of the Federalist Papers, only the papers Hamilton wrote, only the papers originally attributed to Madison, and the papers originally attributed to Madison and the disputed papers. These generated bodies of texts are saved into a tsv file for later use. All resulting models are saved as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file):\n",
    "    papers=getall_papers(file)\n",
    "    authors, papers, numbers = get_authortext(papers)\n",
    "    hamilton, madison, madisonall, allpapers = split_authors(authors, papers)\n",
    "    rnn_model(allpapers)\n",
    "    hamilton_generated=rnn_model(hamilton)\n",
    "    madison_generated=rnn_model(madison)\n",
    "    madison1_generated=rnn_model(madisonall)\n",
    "    save_generated_text(hamilton_generated, madison_generated, madison1_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='federalist papers recurrent neural network')\n",
    "    parser.add_argument('--path', type=str, default=\"allpapers.tsv\",\n",
    "                        help='path to federalist papers dataset')\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    main(args.path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
