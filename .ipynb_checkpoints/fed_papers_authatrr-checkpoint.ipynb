{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Attribution Models for the Federalist Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook compares various author attribution techniques previously tested on the Federalist Papers, using only the papers themselves and not outside sources of writing. The goal is to reproduce similar results and create models for use in later analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed packages\n",
    "import argparse\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Gutenberg supplies a plain text document of all of the federalist papers at: http://www.gutenberg.org/cache/epub/18/pg18.txt. That document has been added to this repository as \"18.txt\". Since the actualy papers are contained within the one document that includes text from Project Gutenberg and information on each paper, the text itself must be extracted from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_texts(fed_papers): #splits project gutenberg data into text files by paper\n",
    "    with open(fed_papers) as fedpapers:\n",
    "        papers=fedpapers.read()\n",
    "        splits = re.split(r'FEDERALIST\\.? No\\.?', papers)\n",
    "        for i in range(len(splits)):\n",
    "            num=str(i)\n",
    "            out = open(f\"fednum{num}.txt\", \"w\")\n",
    "            out.write(splits[i])\n",
    "            out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individualized papers are then read in to create a tsv file recording the paper number, the author, and the text. John Jay's papers are disregarded for the analyses, since the disputed papers are known to be written by Hamilton or Madison. The papers collaborated on by are also disregarded, as they would only add noise to the attribution models. In order ro isolate just the body text, the text of the paper is split at the opening line and the signature, which are consistent across each paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known_authors():\n",
    "    labels=open(\"labels.tsv\", \"w\")\n",
    "    writer=csv.writer(labels, delimiter='\\t')\n",
    "    for i in range(1,87):\n",
    "        with open(f\"fednum{i}.txt\") as paper:\n",
    "            paper=paper.read()\n",
    "            paper=paper.replace('\\n', ' ')\n",
    "            text=re.split(r'To the People of the State of New York', paper)\n",
    "            strip=re.split(r'PUBLIUS', text[1])\n",
    "            if \"HAMILTON OR MADISON\" in text[0]:\n",
    "                writer.writerow([i, \"Unknown\", strip[0]])\n",
    "            elif \"58\" in text[0]: #Project Gutenberg classifies this disputed paper as Madison\n",
    "                writer.writerow([i, \"Unknown\", strip[0]])\n",
    "            elif \"HAMILTON AND MADISON\" in text[0]: #ignore collaborated texts\n",
    "                quit\n",
    "            elif \"HAMILTON\" in text[0]:\n",
    "                writer.writerow([i, \"Hamilton\", strip[0]])\n",
    "            elif \"MADISON\" in text[0]:\n",
    "                writer.writerow([i, \"Madison\", strip[0]])\n",
    "    labels=open(\"labels.tsv\", \"r\")\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lists of the texts and their corresponding authors are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authortext(labels):\n",
    "    authors=[]\n",
    "    papers=[]\n",
    "    numbers=[]\n",
    "    for line in labels:\n",
    "        fields = line.strip().split(\"\\t\")\n",
    "        authors.append(fields[1])\n",
    "        papers.append(fields[2])\n",
    "        numbers.append(fields[0])\n",
    "    return authors, papers, numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lists are then split into the training and testing data, which represents the known papers and the disputed papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(authors,papers, numbers):\n",
    "    trainauth=[]\n",
    "    traintext=[]\n",
    "    test=[]\n",
    "    testnums=[]\n",
    "    for i, author in enumerate(authors):\n",
    "        if author == 'Unknown':\n",
    "            test.append(papers[i])\n",
    "            testnums.append(numbers[i])\n",
    "        else:\n",
    "            trainauth.append(author)\n",
    "            traintext.append(papers[i])\n",
    "    return trainauth, traintext, test, testnums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostellar & Wallace (1964) is the original study in using quantitative analyses to determine the author of the twelve disputed Federalist Papers. They outlined a list of 70 function words that could be used to discriminate between authors based on frequency. A text file containing these words has been previously composed and added to this repository. This list of words will be used in the models to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_function_words(resource_path): #reads in words from text file separated by new line\n",
    "    f_words = []\n",
    "    with open(resource_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                f_words.append(line.lower().strip())\n",
    "    return f_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature vectors are created for counts of the function words for each text. The authors are assigned a class index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_vecs(train_texts, train_labels, test, function_words): \n",
    "    #create matrix of zeros where row corresponds to paper and column to function word\n",
    "    paper_features = np.zeros((len(train_texts),len(function_words)), dtype=np.int)\n",
    "    test_features = np.zeros((len(test),len(function_words)), dtype=np.int)\n",
    "    \n",
    "    #populate matrix with counts for each function word for each paper\n",
    "    for i,text in enumerate(train_texts):\n",
    "        for j,function_word in enumerate(function_words):\n",
    "            text_tokens = text.lower().split()\n",
    "            count = len([w for w in text_tokens if w == function_words[j]])\n",
    "            paper_features[i,j] = count\n",
    "    \n",
    "    for i,text in enumerate(test):\n",
    "        for j,function_word in enumerate(function_words):\n",
    "            text_tokens = text.lower().split()\n",
    "            count = len([w for w in text_tokens if w == function_words[j]])\n",
    "            test_features[i,j] = count\n",
    "    \n",
    "    # load author data into a label array, assigning a class index per unique author\n",
    "    labels = np.zeros(len(train_texts), dtype=np.int)\n",
    "    unique_authors=np.unique(train_labels)\n",
    "    classindex=range(0,len(unique_authors))\n",
    "    for i, author in enumerate(train_labels):\n",
    "        for j,unique in enumerate(unique_authors):\n",
    "            if author == unique: \n",
    "                labels[i]=classindex[j]\n",
    "    \n",
    "    return paper_features, labels, unique_authors, test_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Na√Øve Bayes model is trained on the known papers using the feature vector and corresponding author labels. This model was chosen because the original methods implemented by Mosteller and Wallace (1964) utilized Bayesian statistics and the general model has become one of the most commonly used for class prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(paper_features, labels, unique_authors, test_features, test_numbers):\n",
    "    #predict author for test data using naive bayes model trained on training data\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(paper_features, labels)\n",
    "    score=clf.score(paper_features, labels)\n",
    "    print(f\"Naive Bayes Training Accuracy: {score}\")\n",
    "    preds=clf.predict(test_features)\n",
    "    print(\"Naive Bayes Predictions\")\n",
    "    for i, pred in enumerate(preds):\n",
    "        num=test_numbers[i]\n",
    "        if pred == 0:\n",
    "            auth = unique_authors[0]\n",
    "        else:\n",
    "            auth = unique_authors[1]\n",
    "        print(f\"Federalist No {num} predicted author: {auth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A K-Nearest Neighbors model is also fit to the data, to explore a common similarity-based metric. However, since the feature vectors are made up only of the 70 functoin words, instead of a more sophisticated and complex approach such as TF-IDF vectorization, this model will likely be limited in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(paper_features, labels, unique_authors, test_features, test_numbers):  \n",
    "    #predict author for test data using k-nearest neighbor model trained on training data     \n",
    "    knn = KNeighborsClassifier(n_neighbors=2, weights='distance', algorithm = 'brute')\n",
    "    mod=knn.fit(paper_features, labels)\n",
    "    score=knn.score(paper_features, labels)\n",
    "    print(f\"KNN Training Accuracy: {score}\")\n",
    "    preds=mod.predict(test_features)\n",
    "    print(\"K-Means Predictions\")\n",
    "    for i, pred in enumerate(preds):\n",
    "        num=test_numbers[i]\n",
    "        if pred == 0:\n",
    "            auth = unique_authors[0]\n",
    "        else:\n",
    "            auth = unique_authors[1]\n",
    "        print(f\"Federalist No {num} predicted author: {auth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a Support Vector Machine model is trained. For the SVM prediction model only 3 of the 70 words defined by Mostellar & Wallace (1964) are used. This is based on Bosch & Smith (1998), which established that a hyperplane could be defined using only three of the words, as, our, and upon, that correctly predicted all of the disputed papers to have been written by Madison (which is the standard and accepted conclusion).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_mod(paper_features, labels, unique_authors, test_features, test_numbers):\n",
    "    #predict author for test data using support vector machine trained on training data   \n",
    "    \n",
    "    #use features selected by Bosch & Smith (1998)\n",
    "    train_feats=pd.DataFrame(paper_features)\n",
    "    train_feats=train_feats.iloc[:,[57, 43, 59]]\n",
    "    test_feats=pd.DataFrame(test_features)\n",
    "    test_feats=test_feats.iloc[:,[57, 43, 59]]\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    clf.fit(train_feats, labels)\n",
    "    score=clf.score(train_feats, labels)\n",
    "    print(f\"SVM Training Accuracy: {score}\")\n",
    "    preds=clf.predict(test_feats)\n",
    "    print(\"SVM Predictions\")\n",
    "    for i, pred in enumerate(preds):\n",
    "        num=test_numbers[i]\n",
    "        if pred == 0:\n",
    "            auth = unique_authors[0]\n",
    "        else:\n",
    "            auth = unique_authors[1]\n",
    "        print(f\"Federalist No {num} predicted author: {auth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the above functions are called to read in the data, process it, and make predictions on the disputed papers using the three methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(fed_papers_doc, function_words):\n",
    "    separate_texts(fed_papers_doc)\n",
    "    labels=known_authors()\n",
    "    authors, papers, numbers = get_authortext(labels)\n",
    "    trainauth, traintext, test, testnums = split_data(authors, papers, numbers)\n",
    "    func_words=load_function_words(function_words)\n",
    "    paper_feats, labels, unique_auth, test_feats=feature_vecs(traintext, trainauth, test, func_words)\n",
    "    naive_bayes(paper_feats, labels, unique_auth, test_feats, testnums)\n",
    "    knn(paper_feats, labels, unique_auth, test_feats, testnums)\n",
    "    svm_mod(paper_feats, labels, unique_auth, test_feats, testnums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Training Accuracy: 1.0\n",
      "Naive Bayes Predictions\n",
      "Federalist No 49 predicted author: Madison\n",
      "Federalist No 50 predicted author: Madison\n",
      "Federalist No 51 predicted author: Madison\n",
      "Federalist No 52 predicted author: Madison\n",
      "Federalist No 53 predicted author: Madison\n",
      "Federalist No 54 predicted author: Madison\n",
      "Federalist No 55 predicted author: Hamilton\n",
      "Federalist No 56 predicted author: Madison\n",
      "Federalist No 57 predicted author: Madison\n",
      "Federalist No 58 predicted author: Madison\n",
      "Federalist No 62 predicted author: Madison\n",
      "Federalist No 63 predicted author: Madison\n",
      "KNN Training Accuracy: 1.0\n",
      "K-Means Predictions\n",
      "Federalist No 49 predicted author: Hamilton\n",
      "Federalist No 50 predicted author: Hamilton\n",
      "Federalist No 51 predicted author: Madison\n",
      "Federalist No 52 predicted author: Hamilton\n",
      "Federalist No 53 predicted author: Madison\n",
      "Federalist No 54 predicted author: Madison\n",
      "Federalist No 55 predicted author: Hamilton\n",
      "Federalist No 56 predicted author: Hamilton\n",
      "Federalist No 57 predicted author: Madison\n",
      "Federalist No 58 predicted author: Madison\n",
      "Federalist No 62 predicted author: Hamilton\n",
      "Federalist No 63 predicted author: Madison\n",
      "SVM Training Accuracy: 0.9848484848484849\n",
      "SVM Predictions\n",
      "Federalist No 49 predicted author: Madison\n",
      "Federalist No 50 predicted author: Hamilton\n",
      "Federalist No 51 predicted author: Madison\n",
      "Federalist No 52 predicted author: Madison\n",
      "Federalist No 53 predicted author: Madison\n",
      "Federalist No 54 predicted author: Hamilton\n",
      "Federalist No 55 predicted author: Madison\n",
      "Federalist No 56 predicted author: Hamilton\n",
      "Federalist No 57 predicted author: Madison\n",
      "Federalist No 58 predicted author: Madison\n",
      "Federalist No 62 predicted author: Madison\n",
      "Federalist No 63 predicted author: Madison\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='federalist papers author attribution')\n",
    "    parser.add_argument('--path', type=str, default=\"18.txt\",\n",
    "                        help='path to federalist papers text file')\n",
    "    parser.add_argument('--function_words_path', type=str, default=\"function_words.txt\",\n",
    "                        help='path to the list of words to use as features')\n",
    "    args =     parser.parse_known_args()[0]\n",
    "\n",
    "    main(args.path, args.function_words_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare these results against the widely accepted conclusion that all twelve papers were written by Madison (which was the result of Mostellar & Wallace (1964) and subsequent studies sought to replicate their findings), it appears that the Na√Øve Bayes model produces the highest accuracy on both the training data and the testing, only attributing one paper to Hamilton. The SVM prediction is a close second with slightly lower training accuracy and correctly prediction 9/12 disputed papers. The K-Nearest Neighbor model accurately classifies all of the training data, but performs no better than chance (50%) on the testing data. This could be due to the marked difference in corpora size for Hamilton papers versus Madison papers (i.e. 51 vs. 14). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
